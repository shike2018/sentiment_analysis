{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import io\n",
    "import requests\n",
    "import datetime\n",
    "import ondemand\n",
    "from scipy.stats import pearsonr\n",
    "from pandas.io.common import EmptyDataError\n",
    "from requests.exceptions import HTTPError\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from apiclient.discovery import build\n",
    "#from oauth2client.service_account import ServiceAccountCredentials\n",
    "import httplib2\n",
    "from oauth2client import client\n",
    "from oauth2client import file\n",
    "from oauth2client import tools\n",
    "####Google Analytics Module\n",
    "class GA(object): \n",
    "    def __init__(self, start_date, end_date, symbol):\n",
    "        self.page_view = None\n",
    "        self.symbol = symbol\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "    def initialize_analyticsreporting(self):\n",
    "        \"\"\"\n",
    "          Initializes the analyticsreporting service object.\n",
    "\n",
    "      Returns:\n",
    "        analytics an authorized analyticsreporting service object.\n",
    "        \"\"\"\n",
    "        SCOPES = ['https://www.googleapis.com/auth/analytics.readonly']\n",
    "        DISCOVERY_URI = ('https://analyticsreporting.googleapis.com/$discovery/rest')\n",
    "        CLIENT_SECRETS_PATH = 'client_secrets.json' # Path to client_secrets.json file.\n",
    "      # Parse command-line arguments.\n",
    "        parser = argparse.ArgumentParser(\n",
    "          formatter_class=argparse.RawDescriptionHelpFormatter,\n",
    "          parents=[tools.argparser])\n",
    "        flags = parser.parse_args([])\n",
    "\n",
    "      # Set up a Flow object to be used if we need to authenticate.\n",
    "        flow = client.flow_from_clientsecrets(\n",
    "          CLIENT_SECRETS_PATH, scope=SCOPES,\n",
    "          message=tools.message_if_missing(CLIENT_SECRETS_PATH))\n",
    "\n",
    "      # Prepare credentials, and authorize HTTP object with them.\n",
    "      # If the credentials don't exist or are invalid run through the native client\n",
    "      # flow. The Storage object will ensure that if successful the good\n",
    "      # credentials will get written back to a file.\n",
    "        storage = file.Storage('analyticsreporting.dat')\n",
    "        credentials = storage.get()\n",
    "        if credentials is None or credentials.invalid:\n",
    "            credentials = tools.run_flow(flow, storage, flags)\n",
    "        http = credentials.authorize(http=httplib2.Http())\n",
    "\n",
    "      # Build the service object.\n",
    "        analytics = build('analytics', 'v4', http=http, discoveryServiceUrl=DISCOVERY_URI)\n",
    "        return analytics\n",
    "\n",
    "    def get_report(self, analytics, symbol, col_name):\n",
    "        VIEW_ID = '108608708'#barchart.com view_id\n",
    "        #set filter pagePath according to different symbol.\n",
    "        #total_filter = 'ga:pagePath=~(?i)/stocks/quotes/{}/*?'.format(symbol)\n",
    "        #overview_filter = 'ga:pagePath=~(?i)/stocks/quotes/{}/overview/*?'.format(symbol)\n",
    "        #build filter exrepression\n",
    "        base = 'ga:pagePath=~(?i)/stocks/quotes/{}/*?'.format(symbol)\n",
    "        filters = {}\n",
    "        for sub_page in col_name:\n",
    "            if sub_page == 'total_page_views':\n",
    "                filters[sub_page] = 'ga:pagePath=~(?i)/stocks/quotes/{}.*$;ga:pagePath!~(?i)/story.*$;ga:pagePath!~(?i)/stocks/quotes/{}[A-Za-z0-9\\.-].*$'.format(symbol, symbol)\n",
    "            elif sub_page == 'overview':\n",
    "                filters[sub_page] = 'ga:pagePath=~(?i)/stocks/quotes/{}/{}.*$,ga:pagePath=~(?i)/stocks/quotes/{}/$,ga:pagePath=~(?i)/stocks/quotes/{}$,ga:pagePath=~(?i)/stocks/quotes/{}\\?search={}$;ga:pagePath!~(?i)/story.*$'.\\\n",
    "                format(symbol, sub_page, symbol, symbol, symbol, symbol)\n",
    "            else:\n",
    "                filters[sub_page] = 'ga:pagePath=~(?i)/stocks/quotes/{}/{}.*$;ga:pagePath!~(?i)/story.*$'.format(symbol, sub_page)\n",
    "        final_response = {}\n",
    "        trail = len(col_name)%5\n",
    "        num = len(col_name) // 5\n",
    "        for i in range(num):\n",
    "            # Use the Analytics Service Object to query the Analytics Reporting API V4.\n",
    "            response = analytics.reports().batchGet(\n",
    "          body={\n",
    "            'reportRequests': [\n",
    "               {\n",
    "              'viewId': VIEW_ID,\n",
    "              'dateRanges': [{'startDate': self.start_date, 'endDate': self.end_date}],\n",
    "              'metrics': [{'expression': 'ga:pageviews'}],\n",
    "              'filtersExpression' : filters[col_name[i*5 + 0]],\n",
    "              'dimensions':\n",
    "                     [\n",
    "                        {\n",
    "                         'name': 'ga:date'   \n",
    "                        }                         \n",
    "                     ]\n",
    "                },\n",
    "                {\n",
    "              'viewId': VIEW_ID,\n",
    "              'dateRanges': [{'startDate': self.start_date, 'endDate': self.end_date}],\n",
    "              'metrics': [{'expression': 'ga:pageviews'}],\n",
    "              'filtersExpression' : filters[col_name[i*5 + 1]],\n",
    "              'dimensions':\n",
    "                     [\n",
    "                        {\n",
    "                         'name': 'ga:date'   \n",
    "                        }                         \n",
    "                     ]                \n",
    "                    \n",
    "                },\n",
    "                {\n",
    "              'viewId': VIEW_ID,\n",
    "              'dateRanges': [{'startDate': self.start_date, 'endDate': self.end_date}],\n",
    "              'metrics': [{'expression': 'ga:pageviews'}],\n",
    "              'filtersExpression' : filters[col_name[i*5 + 2]],\n",
    "              'dimensions':\n",
    "                     [\n",
    "                        {\n",
    "                         'name': 'ga:date'   \n",
    "                        }                         \n",
    "                     ]                \n",
    "                    \n",
    "                },\n",
    "                {\n",
    "              'viewId': VIEW_ID,\n",
    "              'dateRanges': [{'startDate': self.start_date, 'endDate': self.end_date}],\n",
    "              'metrics': [{'expression': 'ga:pageviews'}],\n",
    "              'filtersExpression' : filters[col_name[i*5 + 3]],\n",
    "              'dimensions':\n",
    "                     [\n",
    "                        {\n",
    "                         'name': 'ga:date'   \n",
    "                        }                         \n",
    "                     ]                \n",
    "                    \n",
    "                },\n",
    "                {\n",
    "              'viewId': VIEW_ID,\n",
    "              'dateRanges': [{'startDate': self.start_date, 'endDate': self.end_date}],\n",
    "              'metrics': [{'expression': 'ga:pageviews'}],\n",
    "              'filtersExpression' : filters[col_name[i*5 + 4]],\n",
    "              'dimensions':\n",
    "                     [\n",
    "                        {\n",
    "                         'name': 'ga:date'   \n",
    "                        }                         \n",
    "                     ]                \n",
    "                    \n",
    "                }\n",
    "               ]\n",
    "          }\n",
    "      ).execute()\n",
    "            if not final_response.get('reports'):\n",
    "                final_response['reports'] = response['reports']\n",
    "            else:\n",
    "                final_response['reports'] += response['reports']\n",
    "        for i in range(trail):\n",
    "            trail_response = analytics.reports().batchGet(\n",
    "                body = { 'reportRequests': [\n",
    "                    {\n",
    "              'viewId': VIEW_ID,\n",
    "              'dateRanges': [{'startDate': self.start_date, 'endDate': self.end_date}],\n",
    "              'metrics': [{'expression': 'ga:pageviews'}],\n",
    "              'filtersExpression' : filters[col_name[num*5 + i]],\n",
    "              'dimensions':\n",
    "                     [\n",
    "                        {\n",
    "                         'name': 'ga:date'   \n",
    "                        }                         \n",
    "                     ]\n",
    "                   }\n",
    "                ]\n",
    "                }).execute()\n",
    "            if not final_response.get('reports'):\n",
    "                final_response['reports'] = trail_response['reports']\n",
    "            else:\n",
    "                final_response['reports'] += trail_response['reports']\n",
    "        return final_response\n",
    "    def json_cut(self, dic, col_name):\n",
    "        temp_result = {}\n",
    "        report = dic['reports']\n",
    "        try:\n",
    "            #get our date and pageview results\n",
    "            for item in report[0].get('data', {}).get('rows', []):\n",
    "                date = item['dimensions'][0][:4] +'-'+ item['dimensions'][0][4:6]+\\\n",
    "                                          '-'+item['dimensions'][0][6:]\n",
    "                temp_result[date] = item['metrics'][0]['values']\n",
    "            df = pd.DataFrame.from_dict(temp_result, orient='index')\n",
    "            df = df.reset_index()\n",
    "            df = df.rename(index=str, columns={'index': 'date', 0: col_name[0]})\n",
    "            for i in range(1, len(report)):\n",
    "                temp_result = {}\n",
    "                for each in report[i].get('data', {}).get('rows', []):\n",
    "                    date = each['dimensions'][0][:4] +'-'+ each['dimensions'][0][4:6]+\\\n",
    "                                          '-'+ each['dimensions'][0][6:]\n",
    "                    temp_result[date] = each['metrics'][0]['values']\n",
    "                temp_df = pd.DataFrame.from_dict(temp_result, orient = 'index')\n",
    "                temp_df = temp_df.reset_index()\n",
    "                temp_df = temp_df.rename(index = str, columns = {'index' : 'date', 0 : col_name[i]})\n",
    "                df = pd.merge(df, temp_df, how = 'outer', on = 'date')\n",
    "            df.insert(loc = 1, column = 'symbol', value = self.symbol)\n",
    "            self.page_view = df\n",
    "        except:\n",
    "            return True\n",
    "    def main(self, symbol):\n",
    "        analytics = self.initialize_analyticsreporting()\n",
    "        col_name = ['total_page_views', 'overview', 'performance', 'interactive-Chart', 'technical-chart', \\\n",
    "                   'technical-analysis', 'cheat-sheet', 'opinion' , 'trading-strategies', 'price-history', 'options', \\\n",
    "                    'volatility-greeks', 'covered-calls', 'naked-puts', 'option-spreads', 'news',\\\n",
    "                   'profile', 'sec-filings', 'competitors', 'comparison', 'earnings-estimates', \\\n",
    "                    'analyst-ratings', 'income-statement', 'cash-flow', 'balance-sheet']\n",
    "        response = self.get_report(analytics, symbol, col_name)\n",
    "        flag = self.json_cut(response, col_name)\n",
    "        if flag:\n",
    "            print(symbol + ' requests pageview failure!!!') \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_symbols = list(pd.read_csv('Highest_change_total.csv')['Symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2019-06-03\n",
    "all_symbols.index('FITB')\n",
    "all_symbols = all_symbols[3935:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-05 10:27:52.770165\n",
      "0\n",
      "FIXD has no pageview data!!!\n",
      "FIXX has no pageview data!!!\n",
      "FKU has no pageview data!!!\n",
      "10\n",
      "FLMN has no pageview data!!!\n",
      "FLN has no pageview data!!!\n",
      "FLNT has no pageview data!!!\n",
      "20\n",
      "FMB has no pageview data!!!\n",
      "FMCIW has no pageview data!!!\n",
      "FMHI has no pageview data!!!\n",
      "30\n",
      "FNK has no pageview data!!!\n",
      "FNKO has no pageview data!!!\n",
      "FNX has no pageview data!!!\n",
      "FNY has no pageview data!!!\n",
      "FOCS has no pageview data!!!\n",
      "40\n",
      "50\n",
      "FPAYW has no pageview data!!!\n",
      "60\n",
      "FRSX has no pageview data!!!\n",
      "FSCT has no pageview data!!!\n",
      "70\n",
      "FSZ has no pageview data!!!\n",
      "FTA has no pageview data!!!\n",
      "FTC has no pageview data!!!\n",
      "FTCS has no pageview data!!!\n",
      "FTDR has no pageview data!!!\n",
      "FTFT has no pageview data!!!\n",
      "80\n",
      "FTGC has no pageview data!!!\n",
      "FTRI has no pageview data!!!\n",
      "FTSL has no pageview data!!!\n",
      "FTSM has no pageview data!!!\n",
      "FTSV has no pageview data!!!\n",
      "FTXG has no pageview data!!!\n",
      "90\n",
      "FTXH has no pageview data!!!\n",
      "FTXL has no pageview data!!!\n",
      "FTXN has no pageview data!!!\n",
      "FTXO has no pageview data!!!\n",
      "FTXR has no pageview data!!!\n",
      "FUV has no pageview data!!!\n",
      "100\n",
      "FWONA has no pageview data!!!\n",
      "FWONK has no pageview data!!!\n",
      "FYC has no pageview data!!!\n",
      "FYT has no pageview data!!!\n",
      "110\n",
      "FYX has no pageview data!!!\n",
      "GAINL has no pageview data!!!\n",
      "120\n",
      "GBLIL has no pageview data!!!\n",
      "130\n",
      "GECCL has no pageview data!!!\n",
      "GECCM has no pageview data!!!\n",
      "GENY has no pageview data!!!\n",
      "140\n",
      "150\n",
      "GLADN has no pageview data!!!\n",
      "GLDI has no pageview data!!!\n",
      "GLIBA has no pageview data!!!\n",
      "GLIBP has no pageview data!!!\n",
      "160\n",
      "GMLPP has no pageview data!!!\n",
      "170\n",
      "GNPX has no pageview data!!!\n",
      "GNTY has no pageview data!!!\n",
      "180\n",
      "GPAQW has no pageview data!!!\n",
      "190\n",
      "GRID has no pageview data!!!\n",
      "GRIN has no pageview data!!!\n",
      "GRNQ has no pageview data!!!\n",
      "2019-06-05 11:19:34.653823\n",
      "2019-06-05 11:21:04.661853\n",
      "0\n",
      "GSHD has no pageview data!!!\n",
      "GSKY has no pageview data!!!\n",
      "GTHX has no pageview data!!!\n",
      "10\n",
      "20\n",
      "30\n",
      "HCAPZ has no pageview data!!!\n",
      "40\n",
      "50\n"
     ]
    },
    {
     "ename": "HttpError",
     "evalue": "<HttpError 429 when requesting https://analyticsreporting.googleapis.com/v4/reports:batchGet?alt=json returned \"Quota Error: profileId 108608708 has exceeded the daily request limit.\">",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-580c7a63a213>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mga\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'2016-08-01'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'2016-12-31'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mga\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mga\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_view\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' has no pageview data!!!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-cc462094c411>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(self, symbol)\u001b[0m\n\u001b[1;32m    198\u001b[0m                    \u001b[0;34m'profile'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sec-filings'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'competitors'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'comparison'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'earnings-estimates'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m                     'analyst-ratings', 'income-statement', 'cash-flow', 'balance-sheet']\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalytics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mflag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_cut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-cc462094c411>\u001b[0m in \u001b[0;36mget_report\u001b[0;34m(self, analytics, symbol, col_name)\u001b[0m\n\u001b[1;32m    131\u001b[0m                      [\n\u001b[1;32m    132\u001b[0m                         {\n\u001b[0;32m--> 133\u001b[0;31m                          \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ga:date'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m                         }                         \n\u001b[1;32m    135\u001b[0m                      ]                \n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/googleapiclient/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    840\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 842\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 429 when requesting https://analyticsreporting.googleapis.com/v4/reports:batchGet?alt=json returned \"Quota Error: profileId 108608708 has exceeded the daily request limit.\">"
     ]
    }
   ],
   "source": [
    "# pull pageview data here\n",
    "to_drop_symbol = []\n",
    "for i in range(0, len(all_symbols)//200 + 1):  # In order to aviod Google Analytics quota or limitation. I need to run by group.\n",
    "    if i > 0:\n",
    "        time.sleep(90) # Aviod quota limitation\n",
    "    temp_symbols = all_symbols[i*200 : (i+1)*200]\n",
    "    flag = i\n",
    "    print(datetime.datetime.now())\n",
    "    for i, item in enumerate(temp_symbols):  \n",
    "        #print('over here!!!') #test\n",
    "        if i % 2 == 0:\n",
    "            time.sleep(1)\n",
    "        if i % 10 == 0:\n",
    "            print(i)\n",
    "        ga = GA('2016-08-01', '2016-12-31', item)\n",
    "        ga.main(item)\n",
    "        if ga.page_view is None:\n",
    "            print(item + ' has no pageview data!!!')\n",
    "            to_drop_symbol.append(item)\n",
    "            continue\n",
    "        if ga.page_view.empty:\n",
    "            print(item + ' has no pageview data!!!')\n",
    "            to_drop_symbol.append(item) #I need to drop the symbols if they have no pageview data.\n",
    "            continue\n",
    "        ga.page_view = ga.page_view.fillna(0)\n",
    "        ga.page_view.to_csv('pageview_daily_2016/pageview_daily_{}.csv'.format(item.lower()), index = False)\n",
    "        #pageview_daily[item] = ga.page_view\n",
    "    print(datetime.datetime.now())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using this function to fill missed date with zero pageviews\n",
    "def temp_parallel(item):\n",
    "    try:\n",
    "        df = pd.read_csv('pageview_daily_2017/pageview_daily_{}.csv'.format(item.lower()))\n",
    "    except FileNotFoundError:\n",
    "        return\n",
    "    df.fillna(0, inplace = True)\n",
    "    idx = pd.date_range('2017-07-01', '2017-12-31')\n",
    "    df.index = pd.DatetimeIndex(df['date'])\n",
    "    df = df.reindex(idx, fill_value = 0)\n",
    "    df.rename(index = str, columns = {'date' : 'to_drop'}, inplace = True)\n",
    "    df.drop(['to_drop'], axis = 1, inplace = True)\n",
    "    df.reset_index(inplace = True)\n",
    "    df['index'] = pd.to_datetime(df['index']).dt.date\n",
    "    df.rename(index = str, columns = {'index' : 'date'}, inplace = True)\n",
    "    df['symbol'] = item\n",
    "    df.to_csv('pageview_daily_2017/pageview_daily_{}.csv'.format(item.lower()), index = False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool as ThreadPool\n",
    "from multiprocessing import Manager\n",
    "pool = ThreadPool(10)\n",
    "#pool.map(temp_parallel, symbols)  #important, no run except permission\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine data in 2017 with 2018\n",
    "for item in symbols:\n",
    "    try:\n",
    "        df1 = pd.read_csv('pageview_daily_2017/pageview_daily_{}.csv'.format(item.lower()))\n",
    "        df2 = pd.read_csv('pageview_daily_2018/pageview_daily_{}.csv'.format(item.lower()))\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    df = pd.concat([df1, df2], sort = False)\n",
    "    df = df.to_csv('pageview_daily/pageview_daily_{}.csv'.format(item.lower()), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool as ThreadPool\n",
    "from multiprocessing import Manager\n",
    "class Get_Summary(object):\n",
    "    def __init__(self, start_date, end_date):\n",
    "        #we need to catch error, if dates are illeagle\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.page_views = dict()\n",
    "        self.summary_res = pd.DataFrame([], columns = ['Symbol', 'startDate', 'endDate', 'Total Page View', \\\n",
    "                                          'Average Daily Page View', 'Std Daily Page View', 'Lowest Daily Page View', \\\n",
    "                                         '25% Daily Page View', '50% Daily Page View', '75% Daily Page View', \\\n",
    "                                         'Highest Daily Page View', 'Market_Cap(billion)'])\n",
    "    def get_mkt_cap(self, symbol):\n",
    "\n",
    "        url_shares = 'https://ondemand.websol.barchart.com/getFinancialHighlights.csv?apikey=OnDemand&symbols={}&fields=lastQtrEPS%2CannualEPS%2CttmEPS'.\\\n",
    "                format(symbol)\n",
    "        try:\n",
    "            fundmantal = pd.read_csv(url_shares)\n",
    "            if fundmantal.empty:\n",
    "                print(symbol + ' does not have public market value!!!')\n",
    "                return None\n",
    "            else:\n",
    "                shares = fundmantal.loc[0, 'sharesOutstanding']/1000000 # Change to billion\n",
    "        except HTTPError as he:\n",
    "            print('getFinancialHighlights API Error!!! Run again')\n",
    "        url_price = 'https://ondemand.websol.barchart.com/getHistory.csv?apikey=OnDemand&symbol={}&type=daily&startDate={}&endDate={}&maxRecords=1000&interval=60&order=asc&sessionFilter=EFK&splits=true&dividends=true&volume=sum&nearby=1&jerq=true&exchange=NYSE%2CAMEX%2CNASDAQ&backAdjust=false&daysToExpiration=1&contractRoll=expiration'.\\\n",
    "                format(symbol, self.end_date, self.end_date)\n",
    "        try:\n",
    "            price = pd.read_csv(url_price)\n",
    "            if price.empty:\n",
    "                print(symbol + ' has no trading price on {}, the mkt_cap is obtained by getfinancialhighlight API'.format(end_date))\n",
    "                mkt = pd.read_csv('https://ondemand.websol.barchart.com/getFinancialHighlights.csv?apikey=ondemand&symbols={}'.format(symbol))\n",
    "                return float(mkt['marketCapitalization'])/1000000\n",
    "        except:\n",
    "            print(symbol + ' has no trading price on {}, the mkt_cap is obtained by getfinancialhighlight API'.format(self.end_date))\n",
    "            mkt = pd.read_csv('https://ondemand.websol.barchart.com/getFinancialHighlights.csv?apikey=ondemand&symbols={}'.format(symbol))\n",
    "            return float(mkt['marketCapitalization'])/1000000            \n",
    "        price_end = price['close']\n",
    "        return float(price_end)*shares\n",
    "    def combine_page(self):\n",
    "        midcap = pd.read_csv('mid_cap_symbols.csv')\n",
    "        all_symbols = list(midcap['Symbol'])\n",
    "        for item in all_symbols:\n",
    "            try:\n",
    "                temp = self.page_views[item]\n",
    "            except:\n",
    "                continue\n",
    "            self.summary_res = pd.concat([self.summary_res, temp], sort = False)\n",
    "    def get_data(self, item):\n",
    "        try:\n",
    "            pageview_daily = pd.read_csv('pageview_daily/pageview_daiyl_{}.csv'.format(item.lower()))\n",
    "        except FileNotFound:\n",
    "            return\n",
    "        total = pageview_daily['total_page_views'].apply(int)\n",
    "        total_pageview = sum(total)\n",
    "        temp_res = pd.DataFrame(total.describe()).T\n",
    "        temp_res = temp_res.reset_index()\n",
    "        temp_res['startDate'] = ga.start_date\n",
    "        temp_res['endDate'] = ga.end_date\n",
    "        temp_res['total_page'] = total_pageview\n",
    "        temp_res['symbol'] = ga.symbol\n",
    "        temp_res['mkt(billion)'] = self.get_mkt_cap(item)\n",
    "        temp_res = temp_res.drop(['index', 'count'], axis = 1)\n",
    "        temp_res = temp_res[['symbol', 'startDate', 'endDate', 'total_page', 'mean', 'std', 'min', '25%', '50%', '75%',\\\n",
    "                            'max', 'mkt(billion)']]\n",
    "        temp_res['mean'] = temp_res['mean'].round(2)\n",
    "        temp_res['std'] = temp_res['std'].round(2)\n",
    "        if temp_res['mkt(billion)'].isnull:\n",
    "            pass\n",
    "        else:\n",
    "            temp_res['mkt(billion)'] = temp_res['mkt(billion)'].round(2)\n",
    "        temp_res = temp_res.rename(index = str, columns = {'symbol' : 'Symbol', 'total_page' : 'Total Page View',\\\n",
    "                                                          'mean' : 'Average Daily Page View', \\\n",
    "                                                          'std' : 'Std Daily Page View', 'min' : 'Lowest Daily Page View',\\\n",
    "                                                          '25%' : '25% Daily Page View', '50%' : '50% Daily Page View', \\\n",
    "                                                          '75%' :  '75% Daily Page View', 'max' : 'Highest Daily Page View',\\\n",
    "                                                          'mkt(billion)' : 'Market_Cap(billion)'})\n",
    "        self.page_views[item] = temp_res\n",
    "    def main(self, symbols):\n",
    "        \"\"\"\n",
    "        to_drop_symbol = self.get_page_view(symbols)\n",
    "        #drop here\n",
    "        for item in to_drop_symbol:\n",
    "            symbols.remove(item)\n",
    "        self.combine_page(symbols)        \n",
    "        \"\"\"\n",
    "        print(datetime.datetime.now())\n",
    "        pool = ThreadPool(30)\n",
    "        pool.map(self.get_data, all_symbols) #important, no run except permission\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        self.combine_page()\n",
    "        print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine pageview and price.\n",
    "from multiprocessing import Pool as ThreadPool\n",
    "from multiprocessing import Manager\n",
    "pageview_price = Manager().dict()\n",
    "def page_price_parallel(item):\n",
    "    temp_pageview_price = pd.DataFrame([])\n",
    "    try:\n",
    "        pageview_daily = pd.read_csv('pageview_daily/pageview_daily_{}.csv'.format(item.lower()))\n",
    "    except FileNotFoundError:\n",
    "        return \n",
    "    temp_pageview_price['Date'] = pageview_daily['date']\n",
    "    temp_pageview_price['Symbol'] = item\n",
    "    temp_pageview_price['PageView'] = pageview_daily['total_page_views'].apply(int)\n",
    "    url_price = 'https://ondemand.websol.barchart.com/getHistory.csv?apikey=OnDemand&symbol={}&type=daily&startDate={}&endDate={}&maxRecords=1000&interval=60&order=asc&sessionFilter=EFK&splits=true&dividends=true&volume=sum&nearby=1&jerq=true&exchange=NYSE%2CAMEX%2CNASDAQ&backAdjust=false&daysToExpiration=1&contractRoll=expiration'.\\\n",
    "                format(item, '2017-07-01', '2018-12-31')\n",
    "    try:\n",
    "        whole_price = pd.read_csv(url_price)\n",
    "    except EmptyDataError:\n",
    "        print('{} has no price data during this time!!!'.format(item))\n",
    "        whole_price = pd.DataFrame([], columns = ['tradingDay', 'close'])\n",
    "    whole_price.rename(index = str, columns = {'tradingDay' : 'Date', 'close' : 'Close Price'}, inplace = True)\n",
    "    temp_pageview_price = temp_pageview_price.merge(whole_price, how = 'outer', on = 'Date')\n",
    "    price_col_name = list(whole_price.columns.values)\n",
    "    name_to_drop = [n for n in price_col_name if n not in ['Date', 'Close Price']]\n",
    "    temp_pageview_price.drop(name_to_drop, axis = 1, inplace = True)\n",
    "    #drop observations for no trading.\n",
    "    temp_pageview_price = temp_pageview_price[pd.notnull(temp_pageview_price['Close Price'])]\n",
    "    temp = temp_pageview_price['PageView'] \n",
    "    temp_std = temp.rolling(100).std()\n",
    "    temp_pageview_price['PageView_Change_in_Std_1'] = \\\n",
    "        ((temp.rolling(1).mean() - temp.rolling(100).mean().shift(1))/temp_std.shift(1)).round(2)\n",
    "    temp_pageview_price['PageView_Change_in_Std_3'] = \\\n",
    "        ((temp.rolling(3).mean() - temp.rolling(100).mean().shift(3))/temp_std.shift(3)).round(2)\n",
    "    pageview_price[item] = temp_pageview_price\n",
    "    temp_pageview_price.to_csv('pageview_price/pageview_price_{}.csv'.format(item.lower()), index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMITF has no price data during this time!!!\n",
      "HMNY has no price data during this time!!!\n",
      "FOX has no price data during this time!!!\n",
      "FOXA has no price data during this time!!!\n",
      "RGSE has no price data during this time!!!\n",
      "OHGI has no price data during this time!!!\n",
      "LINK has no price data during this time!!!\n",
      "THST has no price data during this time!!!\n"
     ]
    }
   ],
   "source": [
    "pool = ThreadPool(20)\n",
    "pool.map(page_price_parallel, symbols)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_symbols = all_symbols[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6110"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in all_symbols:\n",
    "    #fix price change forward problem\n",
    "    try:\n",
    "        df = pd.read_csv('pageview_price/pageview_price_{}.csv'.format(each.lower()))\n",
    "    except:\n",
    "        continue\n",
    "    df.drop(['PageView_level_in_Std'], axis = 1, inplace = True) \n",
    "    temp = df['PageView']\n",
    "    temp_res = ((temp - temp.rolling(100).mean())/temp.rolling(100).std()).round(2)\n",
    "    df.insert(loc = 4, column = 'PageView_level_in_Std(rolling_100days)', value = temp_res)\n",
    "    temp2 = df['PageView_level_in_Std(rolling_100days)']\n",
    "    df.insert(loc = 5, column = 'PageView_3Days_in_Std', value = temp2.rolling(3).mean().round(2))\n",
    "    df['PageView_5Days_in_Std'] = temp2.rolling(5).mean().round(2)\n",
    "    df['PageView_30Days_in_Std'] = temp2.rolling(30).mean().round(2)\n",
    "    df.insert(loc = 10, column = 'Close Price Change_Forward_3day', value = df['Close Price'].diff(3))\n",
    "    df.insert(loc = 11, column = 'Close Price Percent Change_Forward_3day(%)', \\\n",
    "          value = (df['Close Price'].pct_change(3)*100).round(2))\n",
    "    df['Close Price Change_Forward_3day'] = df['Close Price Change_Forward_3day'].shift(-3)\n",
    "    df['Close Price Percent Change_Forward_3day(%)'] = df['Close Price Percent Change_Forward_3day(%)'].shift(-3)   \n",
    "    df.to_csv('pageview_price/pageview_price_{}.csv'.format(each.lower()), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool as ThreadPool\n",
    "from multiprocessing import Manager\n",
    "class Generate_Leaders():\n",
    "    def __init__(self): \n",
    "        self.pageview_change = Manager().dict()\n",
    "        self.my_date = None\n",
    "    def get_mkt_cap(self, symbol, start_date, end_date):\n",
    "\n",
    "        url_shares = 'https://ondemand.websol.barchart.com/getFinancialHighlights.csv?apikey=OnDemand&symbols={}&fields=lastQtrEPS%2CannualEPS%2CttmEPS'.\\\n",
    "                format(symbol)\n",
    "        try:\n",
    "            fundmantal = pd.read_csv(url_shares)\n",
    "            if fundmantal.empty:\n",
    "                print(symbol + ' does not have public market value!!!')\n",
    "                return None\n",
    "            else:\n",
    "                shares = fundmantal.loc[0, 'sharesOutstanding']/1000000 # Change to billion\n",
    "        except HTTPError as he:\n",
    "            print('getFinancialHighlights API Error!!! Run again')\n",
    "        url_price = 'https://ondemand.websol.barchart.com/getHistory.csv?apikey=OnDemand&symbol={}&type=daily&startDate={}&endDate={}&maxRecords=1000&interval=60&order=asc&sessionFilter=EFK&splits=true&dividends=true&volume=sum&nearby=1&jerq=true&exchange=NYSE%2CAMEX%2CNASDAQ&backAdjust=false&daysToExpiration=1&contractRoll=expiration'.\\\n",
    "                format(symbol, end_date, end_date)\n",
    "        try:\n",
    "            price = pd.read_csv(url_price)\n",
    "            if price.empty:\n",
    "                print(symbol + ' has no trading price on {}, the mkt_cap is obtained by getfinancialhighlight API'.format(end_date, start_date))\n",
    "                mkt = pd.read_csv('https://ondemand.websol.barchart.com/getFinancialHighlights.csv?apikey=ondemand&symbols={}'.format(symbol))\n",
    "                return float(mkt['marketCapitalization'])/1000000\n",
    "        except:\n",
    "            print(symbol + ' has no trading price on {}, the mkt_cap is obtained by getfinancialhighlight API'.format(end_date, start_date))\n",
    "            mkt = pd.read_csv('https://ondemand.websol.barchart.com/getFinancialHighlights.csv?apikey=ondemand&symbols={}'.format(symbol))\n",
    "            return float(mkt['marketCapitalization'])/1000000            \n",
    "        price_end = price['close']\n",
    "        return float(price_end)*shares\n",
    "    def highest_pageview(self, start_date, end_date):\n",
    "        all_symbols = list(pd.read_csv('Highest_change_total.csv')['Symbol'])\n",
    "        to_combine = pd.DataFrame([], columns = ['Symbol', 'startDate', 'endDate', 'Total Page View', 'Market_Cap(billion)'])\n",
    "        for i, item in enumerate(all_symbols):\n",
    "            try:\n",
    "                pageview_daily = pd.read_csv('pageview_daily/pageview_daily_{}.csv'.format(item.lower()))\n",
    "            except FileNotFoundError:\n",
    "                print('{} has no pageview data!!!'.format(item))\n",
    "                continue\n",
    "            pageview_daily['datetime'] =  pd.to_datetime(pageview_daily['date'])\n",
    "            pageview_daily = pageview_daily.set_index(['datetime'])\n",
    "            start_datetime = pd.to_datetime(start_date)\n",
    "            end_datetime = pd.to_datetime(end_date)\n",
    "            #print(start_date)\n",
    "            pageview_daily = pageview_daily.loc[start_datetime : end_datetime]\n",
    "            #print(pageview_daily)\n",
    "            total = pageview_daily['total_page_views'].apply(int)\n",
    "            total_pageview = sum(total)\n",
    "            #temp_res = pd.DataFrame(total.describe()).T\n",
    "            temp_res = pd.DataFrame([], columns = ['Symbol', 'startDate', 'endDate', 'Total Page View', 'Market_Cap(billion)'])\n",
    "            #temp_res = temp_res.reset_index()\n",
    "            temp_res.loc[0, 'Symbol'] = item\n",
    "            temp_res['startDate'] = start_date\n",
    "            temp_res['endDate'] = end_date\n",
    "            temp_res['Total Page View'] = total_pageview\n",
    "            temp_res['Market_Cap(billion)'] = self.get_mkt_cap(item, start_date, end_date)\n",
    "            if temp_res['Market_Cap(billion)'].isnull().values.any():\n",
    "                pass\n",
    "            else:\n",
    "                temp_res['Market_Cap(billion)'] = temp_res['Market_Cap(billion)'].round(2)\n",
    "            to_combine = pd.concat([to_combine, temp_res])\n",
    "        #to_combine['Market_Cap(billion)'] = to_combine['Market_Cap(billion)'].round(2)\n",
    "        to_combine.reset_index(inplace = True)\n",
    "        to_combine.drop(['index'], axis = 1, inplace = True)\n",
    "        to_combine.sort_values(by = ['Total Page View'], ascending = False, inplace = True)\n",
    "        return to_combine\n",
    "    def parallel(self, item):\n",
    "        try:\n",
    "            pageview_price = pd.read_csv('pageview_price/pageview_price_{}.csv'.format(item.lower()))\n",
    "        except FileNotFoundError:\n",
    "            print('{} has no pageview data!!!'.format(item))\n",
    "            return\n",
    "        if pageview_price.empty:\n",
    "            return\n",
    "        #print(datetime.datetime.now())\n",
    "        temp_res = pageview_price.set_index(['Date'])\n",
    "        try:\n",
    "            temp_res = temp_res.loc[[self.my_date]]\n",
    "            temp_res.reset_index(col_fill = 'Date', inplace = True)\n",
    "        except:\n",
    "            print('{} has no trading on {}!!!'.format(item, self.my_date))\n",
    "            return\n",
    "        temp_res.loc[0, 'Market_Cap(billion)'] = self.get_mkt_cap(item, self.my_date, self.my_date)\n",
    "        if temp_res['Market_Cap(billion)'].isnull().values.any():\n",
    "            pass\n",
    "        else:\n",
    "            temp_res['Market_Cap(billion)'] = temp_res['Market_Cap(billion)'].round(2)\n",
    "        self.pageview_change[item] = temp_res \n",
    "    def highest_change(self, my_date):\n",
    "        all_symbols = list(pd.read_csv('Highest_change_total.csv')['Symbol'])\n",
    "        self.my_date = my_date\n",
    "        print(datetime.datetime.now())\n",
    "        pool = ThreadPool(50)\n",
    "        pool.map(self.parallel, all_symbols)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        print(datetime.datetime.now())\n",
    "        \"\"\"\n",
    "\n",
    "                for i, item in enumerate(all_symbols):\n",
    "            if i % 500 == 0:\n",
    "                print(i)\n",
    "            self.parallel(item)\n",
    "        \"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
